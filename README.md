# ğŸ§  Decifrando a Caixa Preta: Tornando Modelos de IA ExplicÃ¡veis com LIME

Este projeto aplica tÃ©cnicas de *Explainable Artificial Intelligence (XAI)* no contexto de concessÃ£o de crÃ©dito bancÃ¡rio. A partir de um modelo preditivo treinado com o **German Credit Data**, utilizamos a biblioteca [LIME](https://github.com/marcotcr/lime) para gerar explicaÃ§Ãµes locais sobre as decisÃµes do modelo para cada cliente.

---

## ğŸ¯ Objetivo

- Criar um modelo de machine learning para classificar clientes como **bom** ou **mau risco de crÃ©dito**.
- Aplicar a tÃ©cnica **LIME** para gerar explicaÃ§Ãµes individuais, claras e interpretÃ¡veis para decisÃµes de concessÃ£o ou negaÃ§Ã£o de crÃ©dito.
- Atender Ã s necessidades de **clientes, gerentes e Ã³rgÃ£os regulatÃ³rios**, promovendo transparÃªncia e confiabilidade.

---

## ğŸ“Š Dataset: German Credit Data

- **Fonte:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))
- **Registros:** 1000 clientes
- **Atributos:** 20 (como idade, status da conta, histÃ³rico de crÃ©dito, renda, emprego, etc.)
- **Alvo:**  
  - 1 = Bom pagador  
  - 2 = Mau pagador

> Usamos o arquivo `german.data` (versÃ£o original), realizando o prÃ©-processamento com `LabelEncoder`.

---

## âš™ï¸ Tecnologias Utilizadas

- Python 3
- pandas, numpy, matplotlib
- scikit-learn (modelo Random Forest)
- LIME (Local Interpretable Model-Agnostic Explanations)

---

## ğŸ§  Modelo Preditivo

- **Tipo:** ClassificaÃ§Ã£o binÃ¡ria
- **Modelo escolhido:** `RandomForestClassifier`
- **Motivos:**
  - Alta acurÃ¡cia
  - Robustez a dados ruidosos
  - CompatÃ­vel com interpretabilidade via LIME

---

## ğŸ” Etapas do Projeto

1. Carregamento e nomeaÃ§Ã£o das colunas do dataset
2. CodificaÃ§Ã£o dos atributos categÃ³ricos com `LabelEncoder`
3. SeparaÃ§Ã£o treino/teste com `train_test_split`
4. Treinamento de um modelo de floresta aleatÃ³ria
5. AvaliaÃ§Ã£o com `classification_report`
6. AplicaÃ§Ã£o do LIME em uma prediÃ§Ã£o individual
7. GeraÃ§Ã£o de grÃ¡ficos e arquivos explicativos (`.png`, `.html`)

---

## ğŸ“ˆ Exemplo de ExplicaÃ§Ã£o com LIME

Abaixo estÃ¡ um grÃ¡fico real gerado pelo projeto, indicando os fatores que influenciaram a decisÃ£o do modelo para um cliente especÃ­fico.

![lime](images/lime_explanation.png)

---

## â–¶ï¸ Como Executar o Projeto

1. Clone este repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/german-credit-xai-lime.git
cd german-credit-xai-lime
```

2. (Opcional) Crie um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Execute o script principal:
```bash
python src/model_lime_explainer.py
```

5. Verifique os resultados:

- GrÃ¡fico: images/lime_explanation.png
- VersÃ£o interativa: images/lime_explanation.html

# ğŸ“ Estrutura do Projeto

```bash
german-credit-xai-lime/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ german.data
â”œâ”€â”€ images/
â”‚   â””â”€â”€ lime_explanation.png
â”‚   â””â”€â”€ lime_explanation.html
â”œâ”€â”€ src/
â”‚   â””â”€â”€ model_lime_explainer.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ“š ReferÃªncias
- Ribeiro et al. (2016) â€“ "Why Should I Trust You?"
- Dataset: Statlog (German Credit)
- LIME Docs: https://marcotcr.github.io/lime/tutorials.html

## ğŸ‘¨â€ğŸ“ Sobre
Projeto desenvolvido para a disciplina Explainable AI (XAI), com foco em interpretabilidade de modelos preditivos aplicados Ã  concessÃ£o de crÃ©dito bancÃ¡rio.

